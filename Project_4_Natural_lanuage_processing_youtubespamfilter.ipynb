{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "sns.set_context('notebook') \n",
    "sns.set_style('ticks')\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data= pd.read_csv('ytube_spam_trainset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WE FIRST TOKENIZE THE CONTENT INTO WORDS FOR BAG OF WORDS REPRESENTATION\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Now remove stop words \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#convert words to root words -stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#create a function to apply the above process on all of the dataframe\n",
    "\n",
    "def process_text(text):\n",
    "    tokenized = word_tokenize(text)\n",
    "    tokenized_no_punctuation=[word.lower() for word in tokenized if word.isalpha()]\n",
    "    tokenized_no_stopwords=[word for word in tokenized_no_punctuation if word not in stopwords.words('english')]\n",
    "    tokens = [PorterStemmer().stem(word) for word in tokenized_no_stopwords]\n",
    "    return tokens\n",
    "\n",
    "data['tokens']=data['CONTENT'].apply(process_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n",
      "(750, 357)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Step 1\n",
    "bow_transformer= CountVectorizer(analyzer=process_text, stop_words='english',min_df=2,max_df=0.01,max_features=1000).fit(data['CONTENT'])\n",
    "print(len(bow_transformer.vocabulary_))\n",
    "\n",
    "len(bow_transformer.get_feature_names())\n",
    "\n",
    "#Now transform bow_transformer into a sparse matrix by applying .transform method\n",
    "messages_bow= bow_transformer.transform(data['CONTENT'])\n",
    "print(messages_bow.shape)\n",
    "\n",
    "#convert to dataframe\n",
    "\n",
    "import pandas as pd\n",
    "X=pd.DataFrame(messages_bow.toarray(), columns=bow_transformer.get_feature_names())\n",
    "#Step 2\n",
    "\n",
    "#Need to remove top 1% words before this step\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfid_transformer= TfidfTransformer().fit_transform(X)\n",
    "#messages_tfid= tfid_transformer.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=3, error_score='raise',\n",
      "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "       fit_params=None, iid=True, n_jobs=1,\n",
      "       param_grid={'max_depth': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
      "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring=None, verbose=0)\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=33, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "Y= data.iloc[:,4].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0, test_size=0.2)\n",
    "\n",
    "parameters= {'max_depth': np.arange(1,50,1)}\n",
    "dtc= RandomForestClassifier(criterion='entropy')\n",
    "dtc_clv= GridSearchCV(dtc, param_grid= parameters, cv=3)\n",
    "print(dtc_clv)\n",
    "dtc_clv.fit(X_train,y_train)\n",
    "best_depth= dtc_clv.best_estimator_\n",
    "\n",
    "print(best_depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.98       122\n",
      "          1       0.96      0.82      0.88        28\n",
      "\n",
      "avg / total       0.96      0.96      0.96       150\n",
      "\n",
      "[[121   1]\n",
      " [  5  23]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "Y= data.iloc[:,4].values\n",
    "X= data.iloc[:,3]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0, test_size=0.2)\n",
    "\n",
    "#Build a pipeline\n",
    "pipeline= Pipeline([\n",
    "        ('bow',CountVectorizer(analyzer= process_text, stop_words='english',min_df=2,max_features=1000)),\n",
    "        ('tfid',TfidfTransformer()),\n",
    "        ('classifier',RandomForestClassifier(criterion='entropy',max_depth=27))])\n",
    "    \n",
    "pipeline.fit(X_train,y_train)    \n",
    "\n",
    "prediction_NB= pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,prediction_NB))\n",
    "\n",
    "print(confusion_matrix(y_test,prediction_NB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99       350\n",
      "          1       0.99      0.95      0.97        75\n",
      "\n",
      "avg / total       0.99      0.99      0.99       425\n",
      "\n",
      "[[349   1]\n",
      " [  4  71]]\n"
     ]
    }
   ],
   "source": [
    "test_data= pd.read_csv('ytube_spam_testset.csv')\n",
    "\n",
    "y_test = test_data.iloc[:,4].values\n",
    "X_test= test_data.iloc[:,3]\n",
    "\n",
    "pipeline= Pipeline([\n",
    "        ('bow',CountVectorizer(analyzer= process_text, stop_words='english',min_df=2,max_features=1000)),\n",
    "        ('tfid',TfidfTransformer()),\n",
    "        ('classifier',RandomForestClassifier(max_depth=27))])\n",
    "    \n",
    "pipeline.fit(X_test,y_test)    \n",
    "\n",
    "y_pred_final= pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred_final))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred_final))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
